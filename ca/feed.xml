<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://pastells.github.io/ca/feed.xml" rel="self" type="application/atom+xml"/><link href="https://pastells.github.io/ca/" rel="alternate" type="text/html"/><updated>2024-09-30T10:54:09+00:00</updated><id>https://pastells.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">SEPLN 2024</title><link href="https://pastells.github.io/ca/blog/2024/sepln2024/" rel="alternate" type="text/html" title="SEPLN 2024"/><published>2024-09-30T00:00:00+00:00</published><updated>2024-09-30T00:00:00+00:00</updated><id>https://pastells.github.io/blog/2024/sepln2024</id><content type="html" xml:base="https://pastells.github.io/blog/2024/sepln2024/"><![CDATA[<p>La setmana passada vaig participar a la XL SEPLN (la conferència de la societat espanyola de PLN) a Valladolid. Va ser una conferència profitosa. El dimarts vam presentar la ‘overview’ de la tasca <a href="https://detests-dis.github.io/">DETESTS-Dis</a> a IberLEF 2024, <a href="http://journal.sepln.org/sepln/ojs/ojs/index.php/pln/article/view/6620">ja es pot llegir l’article</a>, i el dimecres vaig presentar un <a href="https://besaya.infor.uva.es/sepln24/paper16.pdf">article</a>.</p> <p>Podeu veure més informació sobre els corpus <a href="https://huggingface.co/datasets/CLiC-UB/DETESTS-Dis">a HuggingFace</a>.</p> <p>Tant la tasca com l’article tractaven sobre la detecció d’estereotips racistes en castellà. Tenim dos corpus on s’anota la presència o no d’estereotip, així com si aquest és implícit, entre d’altres anotacions. Un dels corpus constava de comentaris de notícies i l’altre de tuits.</p> <ul> <li> <p>La tasca, on es presentaven diferents equips de forma competitiva, consistia a crear un model per classificar els textos segons si contenien estereotips o no, i si eren implícits o explícits.</p> </li> <li> <p>L’article es basava en la idea que els anotadors sovint necessiten veure el context per saber si hi ha un estereotip o per classificar-lo. En canvi, se sol ometre el context al crear models. La nostra idea era incorporar diferents tipus de context (comentaris anteriors o títol de la notícia) a models tipus BERT fent servir el token [SEP] per passar el context. Al final la cosa no va funcionar molt bé. Per tant, presentavem resultats negatius. Aquesta metodologia no és útil per millorar els models incorporant context, si més no pel nostre cas.</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sepln24-1-480.webp 480w,/assets/img/sepln24-1-800.webp 800w,/assets/img/sepln24-1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/sepln24-1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/valladolid-480.webp 480w,/assets/img/valladolid-800.webp 800w,/assets/img/valladolid-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/valladolid.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/sepln24-2-480.webp 480w,/assets/img/sepln24-2-800.webp 800w,/assets/img/sepln24-2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/sepln24-2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="ML"/><category term="LLMs"/><category term="ML"/><summary type="html"><![CDATA[SEPLN a Valladolid]]></summary></entry><entry><title type="html">Modelització de modes d’Alfvén amb freqüència canviant a l’stellerator TJ-II</title><link href="https://pastells.github.io/ca/blog/2024/modelling_frequency_sweeping_alfven_modes/" rel="alternate" type="text/html" title="Modelització de modes d’Alfvén amb freqüència canviant a l’stellerator TJ-II"/><published>2024-02-27T00:00:00+00:00</published><updated>2024-02-27T00:00:00+00:00</updated><id>https://pastells.github.io/blog/2024/modelling_frequency_sweeping_alfven_modes</id><content type="html" xml:base="https://pastells.github.io/blog/2024/modelling_frequency_sweeping_alfven_modes/"><![CDATA[<p><em>Aquest apunt l’ha escrit originalment en anglès <a href="https://fusion.bsc.es/index.php/2024/02/27/our-new-journal-paper-in-nuclear-fusion-on-modeling-of-frequency-sweeping-alfven-modes/">Mervi Mantsinen</a></em></p> <p><strong>Les inestabilitats alfvéniques impulsades per partícules energètiques suposen un repte per al correcte funcionament dels dispositius de fusió de confinament magnètic.</strong> Aquests modes poden dissipar ions ràpids que condueixen a la introducció de càrregues de calor significatives als components de plasma i a la degradació del confinament global del plasma. Una classe d’inestabilitats d’Alfvén conegudes com a modes propis d’Alfvén de cisalla invertida “reversed shear Alfvén eigenmodes (RSAE)” són particularment perillosos en els dispositius on es donen les condicions perquè aquests apareguin, és a dir, amb perfils de transformació rotacional de cisalla inversa. Les configuracions de cisalla inversa han estat d’interès recentment per la seva millora en la qualitat del confinament magnètic. Tenint en compte això, és necessari un estudi addicional dels RSAE.</p> <p><a href="http://10.0.4.64/0029-5515/54/12/123002">Els RSAE, també anomenats cascades d’Alfvén, s’han pogut observar</a> a <a href="https://www.wikiwand.com/es/TJ-II">l’stellerator TJ-II</a> en descàrregues de plasma d’hidrogen amb diferents configuracions magnètiques. <strong>A la nostra recent publicació a la revista Nuclear Fusion, simulem els esdeveniments en cascada utilitzant els codis <a href="ttps://doi.org/10.1063/1.1590316">STELLGAP</a> i <a href="https://doi.org/10.1063/1.3313818">AE3D</a>, i estudiem la relació entre la freqüència dels modes que formen la cascada i el valor mínim del perfil de transformació rotacional.</strong> Les simulacions prediuen l’aparició d’una cascada de freqüència descendent formada per un conjunt de modes amb nombre de mode poloidal \(m = 6\) i número de mode toroidal \(n = -9\) quan el valor mínim del perfil iota varia entre aproximadament \(1.47 &lt; \iota_{min} &lt; 1.50\), corroborat experimentalment. Els resultats presentats donen suport a la utilitat de l’espectroscòpia MHD, una eina de diagnòstic per mitjà de la qual es pot fer servir el gradient temporal de la freqüència d’una cascada d’Alfvén per determinar la variació en el temps del perfil de transformació rotacional del plasma.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/alfven_simulation_results-480.webp 480w,/assets/img/alfven_simulation_results-800.webp 800w,/assets/img/alfven_simulation_results-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/alfven_simulation_results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Els resultats de la simulació es mostren en negre superposats amb un espectrograma experimental. Les barres d'error assumeixen un error de ± 10% en densitat i ± 0,005 d'error en iota. Reproduït a partir de la Fig 11 in A.G. Ghiozzi et al 2024 Nucl. Fusion 64 036005. </div> <p>L’article complet escrit per Adriana Ghiozzi, juntament amb líder del <a href="https://fusion.bsc.es/">grup de fusió del BSC</a>, Mervi Mantsinen, jo mateix i altres col·laboradors es pot llegir (en obert) al següent enllaç: <a href="https://iopscience.iop.org/article/10.1088/1741-4326/ad1c93">A.G. Ghiozzi <em>et al</em> 2024 <em>Nucl. Fusion</em> 64 036005</a>.</p>]]></content><author><name></name></author><category term="physics"/><category term="fusion"/><category term="physics"/><summary type="html"><![CDATA[Descripció del meu últim article científic]]></summary></entry><entry><title type="html">Depth Up-Scaling</title><link href="https://pastells.github.io/ca/blog/2024/depth_up-scaling/" rel="alternate" type="text/html" title="Depth Up-Scaling"/><published>2024-01-02T00:00:00+00:00</published><updated>2024-01-02T00:00:00+00:00</updated><id>https://pastells.github.io/blog/2024/depth_up-scaling</id><content type="html" xml:base="https://pastells.github.io/blog/2024/depth_up-scaling/"><![CDATA[<p>L’article de <a href="https://arxiv.org/abs/2312.15166">SOLAR10.7B</a> introdueix l’augment de profunditat “Depth Up-Scaling” (DUS) com a competència a la mescla d’experts “mixture-of-experts” (MoE). Es basa en una idea senzilla: agafar un model existent, duplicar-li les capes i preentrenar-lo contínuament “continued pretraining” perquè doni millors resultats. El problema rau en la concatenació de l’última capa del model original amb la primera capa del seu duplicat. Per facilitar l’entrenament continuat del model resultant, els autors eliminen les \(m\) capes finals del model original i les \(m\) capes inicials de la còpia. Mitjançant aquesta tècnica, després de la concatenació, el model pateix una caiguda de rendiment inicial, però es recupera ràpidament durant la fase continuada de preentrenament.</p> <p>Tot i que els autors no ho diuen, eliminar les capes inicials i finals té sentit des del punt de vista de la interpretabilitat mecanicista “mechanistic interpretability” (vegeu el fil <a href="https://transformer-circuits.pub/2021/framework/index.html">d’Anthropic sobre “transformer circuits”</a>), atès que la primera i la darrera capa tenen tasques especialitzades.</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/DUS-480.webp 480w,/assets/img/DUS-800.webp 800w,/assets/img/DUS-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/DUS.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Els autors utilitzen Mistal 7B, amb una arquitectura Llama 2 de 32 capes, com a model base. Prenen \(m=8\), de manera que el model resultant té \(2*(32-8)=48\) capes. No proven altres opcions per \(m\), així que ben segur apareixeran millores del mètode en un futur pròxim.</p> <p>Nous Research ja ha entrenat un model <a href="https://huggingface.co/NousResearch/Nous-Hermes-2-SOLAR-10.7B">Hermes 2 a sobre de SOLAR10.7B</a>, i sembla que funciona molt bé.</p>]]></content><author><name></name></author><category term="ML"/><category term="LLMs"/><category term="ML"/><summary type="html"><![CDATA[Explicació del mètode "Depth Up-Scaling"]]></summary></entry></feed>